## Methods
#### Data Collection: Google Form
1. Experience in coding.
2. Familiarity with project.
3. Ease of understanding the project.
4. Ease of setup and use.
5. Total time taken for setup and usage.

#### Data Collection BTS: Behind The Scenes
1. Time taken to convert some random units without extension.
2. Time to convert the same random units now with extension.

## Materials
The experiment was conducted with the help of:
1. Zoom: For conducting 15-30 minutes session with each participant.
2. Google Form: To anonymously capture experiment results/opinions from each participant.
3. Google Sheet: Observations from our side with respect to each participant.

## Observations
<to be written>



## Conclusion
1. The project still has a scope for further development as we can see the feedbacks from various testers about the copy-paste functionality, unit conversion preferences, weather site compatibility, clickability etc.
2. The maximum time taken in the experiments was on the installation of the software.
3. The use of the extension drastically reduced the conversion time of the units for the users.
4. The extension was very helpful as per the testers and was very intuitive to use.
5. Users do not have to go through of creating a new tab and searching on google when using this extension. They simply just have to highlight the numbers with units. 



## Threats to Validity
The following were observed to be change points for the next testing and validation runs:
1. Unstructured data collection:
   
   Initially, data collection was facilitated using several means and data collation was a huge problem bound to happen. We quickly fixed a central evaluation form, but this is a change point to be kept in mind - to have a structured data collection methodology.
   
2. Insufficient data collection:
   
   Testers might have found varying degrees of difficulty to try and debug the code since the bugs introduced were not uniform, similar or of the same quality. Hence, as testers progressed through the debugging session, they might have felt varying levels of difficulty in a very small amount of time affecting the level of code they wrote.

3. Closed v open environment:
    
    To assertain whether the subjects were actually able to familiarise themselves with the tools, they were also asked if there are able to use it outside the google sheet responses and the internet being an deep, dangerous ocean, it is not always the case where participants are able to find the *correct* website for their taste. Eg: some weather sites do not allow selecting the temperature. Thus, we have to consider whether factoring that open-world time-to-use into our findings makes the analysis generic for the tool or is it just a characteristic of the participant.

4. Cleaner install and easier process:

    Writing the entire, correct procedure in a concise manner in the github README.md would definitely help with the experiment. 

5. Testing steps:

   Writing what the participants are actually intended to do during the test might or might not be beneficial. If the test participant knows what to expect and has already been trying out the tool so as to become "familiar" with it, then do the time data collected during the testing experiment convey the actual ease-of-access time or just practise time - that is a debate point which can be taken into account the next time such experiments are conducted with other participants. In any case, writing an universally understandable project map will help participants know what to expect even if not the actual steps during the experiment itself.


6. Lesser participants as testers:

    The tests conducted was used over a confined audience which belonged to a particular class. Moreover the test was conducted in a very small scale. The test was somewhat biased for a particular group of audience. So the results have this threat to validity in this aspect.
  
In general, writing cleaner code, helper files and a pin-pointed README will help in the future experiments.
